\subsection{Various Payloads}
\label{sec:varioustech}
This section lists evasion techniques applicable to various payloads.

\subsubsection{Payload size}
Some Web application firewalls are configured to skip requests sizes above a set threshold.

AWS WAF limits the \quotes{Maximum size of a web request body that can be inspected for CloudFront, API Gateway, Amazon Cognito, App Runner, and Verified Access protections**} to 64 KB, the \quotes{Maximum size of a web request body that can be inspected for Application Load Balancer and AWS AppSync protections} to 8 KB. \cite{aws/waflimits}

Microsoft Azure allows the configuration of limits on newer Rule Sets:
\begin{quote}
	For Application Gateway v2 Web Application Firewalls running Core Rule Set 3.2, or newer, the maximum request body size enforcement and max file upload size enforcement can be disabled and the Web Application Firewall will no longer reject a request, or file upload, for being too large. When maximum request body size enforcement and max file upload size enforcement are disabled within the Web Application Firewall, Application Gateway's limits determine the maximum size allowable. For more information, see Application Gateway limits. \cite{ms/azurewaflimits}
\end{quote}

With the \quotes{Maximum request inspection limit (WAF SKU)} at a default value of 2 MB with the option to disable the limit completely according to their "Application Gateway limits" document. \cite{ms/appgatewaylimits}

Akamai answers the question \quotes{Can WAF inspect all arguments and values in request body? Is there any limitation on the size of the request?} on their FAQ page with \quotes{By default, the WAF inspects only the first 8KB of a request. It can increase the limit up to 128KB by adding Advanced Metadata.} \cite{akamai/waflimits}

Cloudflare lists a limit of 128 KB in the Cloudflare Rules language definition, with truncation upon reaching it:
\begin{quote}
	All http.request.body.* fields (except http.request.body.size) handle a maximum body size of 128 KB, which means that you cannot define expressions that rely on request body data beyond the first 128 KB. If the request body is larger, the body fields will contain a truncated value and the http.request.body.truncated field will be set to true. The http.request.body.size field will contain the full size of the request without any truncation.

	The maximum body size of 128 KB applies only to the values of HTTP body fields â€” the origin server will still receive the complete request body.
\end{quote}

The ModSecurity recommended configuration sets the following limits for requests:
\begin{quote}
	SecRequestBodyLimit 13107200 \\
	SecRequestBodyNoFilesLimit 131072
\end{quote}
with requests exceeding the limits being rejected by default. \cite{modsec/recconf}

Depending on the specific rule configuration by each service, blowing up requests with filler-data to exceed the inspection limit might make the malicious payload evade detection. Some Web applications might accept request bodies filled with unused data, as long as there is a valid key: value pair inside. If this key: value pair is positioned past the 8 KB limit of the Akamai WAF for instance, the malicious value might go unnoticed.


\subsubsection{Unicode encoding in JSON}
RFC 4627 \quotes{The application/json Media Type for JavaScript Object Notation} states under section 2.5 \quotes{Strings}:
\begin{quote}
	The representation of strings is similar to conventions used in the C
	family of programming languages.  A string begins and ends with
	quotation marks.  All Unicode characters may be placed within the
	quotation marks except for the characters that must be escaped:
	quotation mark, reverse solidus, and the control characters (U+0000
	through U+001F).

	Any character may be escaped.  If the character is in the Basic
	Multilingual Plane (U+0000 through U+FFFF), then it may be
	represented as a six-character sequence: a reverse solidus, followed
	by the lowercase letter u, followed by four hexadecimal digits that
	encode the character's code point.  The hexadecimal letters A though
	F can be upper or lowercase.  So, for example, a string containing
	only a single reverse solidus character may be represented as
	"\u005C". \cite{rfc4627}
\end{quote}
As escaping any character should be possible in a JSON payload, firewall filters that look for a certain sequence of characters might be evaded by escaping one or more characters from that sequence.

	{\color{red} TODO: example, request with unicode replaced \$ }


\subsubsection{Unicode normalization}
The unicode standard defines two types of equivalence between characters: canonical equivalence and compatibility equivalence. The report "Unicode Normalization Forms" by Ken Whistler for Unicode Version 15.1.0 states the following regarding the two types of equivalence:

\begin{quote}
	Canonical equivalence is a fundamental equivalency between characters or sequences of characters which represent the same abstract character, and which when correctly displayed should always have the same visual appearance and behavior. Figure 1 shows this type of equivalence with examples of several subtypes.
	% \\
	% {
	% \includegraphics[width=0.5\textwidth]{caneqb.png}
	% \centering
	% }
	% \\
	\\
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{caneq.png}
		\label{fig:caneq}
		\caption{ Examples of Canonical Equivalence \cite{unicode/normalization}}
	\end{figure}
	\\
	Compatibility equivalence is a weaker type of equivalence between characters or sequences of characters which represent the same abstract character (or sequence of abstract characters), but which may have distinct visual appearances or behaviors. The visual appearances of the compatibility equivalent forms typically constitute a subset of the expected range of visual appearances of the character (or sequence of characters) they are equivalent to. However, these variant forms may represent a visual distinction that is significant in some textual contexts, but not in others. As a result, greater care is required to determine when use of a compatibility equivalent is appropriate. [...] Figure 2 provides examples of compatibility equivalence. \cite{unicode/normalization}
	\\
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{compeq.png}
		\label{fig:compeq}
		\caption{ Examples of Canonical Equivalence \cite{unicode/normalization}}
	\end{figure}
	\\
\end{quote}

Normalization Forms are further described in the report:
\begin{quote}
	Unicode Normalization Forms are formally defined normalizations of Unicode strings which make it possible to determine whether any two Unicode strings are equivalent to each other. Depending on the particular Unicode Normalization Form, that equivalence can either be a canonical equivalence or a compatibility equivalence.

	Essentially, the Unicode Normalization Algorithm puts all combining marks in a specified order, and uses rules for decomposition and composition to transform each string into one of the Unicode Normalization Forms. A binary comparison of the transformed strings will then determine equivalence.

	The four Unicode Normalization Forms are summarized in Table 1. \cite{unicode/normalization}
	\begin{table}[h]
		\centering
		\label{tab:normform}
		\caption{ Normalization Forms }
		\begin{tabular}{ |c|c| }
			\hline
			Form                         & Description                       \\
			\hline
			\hline
			Normalization Form D (NFD)   & Canonical Decomposition           \\
			\hline
			Normalization Form C (NFC)   & Canonical Decomposition,          \\
			                             & followed by Canonical Composition \\
			\hline
			Normalization Form KD (NFKD) & Compatibility Decomposition       \\
			Normalization Form KC (NFKC) & Compatibility Decomposition,      \\
			                             & followed by Canonical Composition \\
			\hline
		\end{tabular}
	\end{table}
\end{quote}
In the context of Firewall Evasion, if inputs on the web server are normalized using one of the afromentioned Normalization Forms during processing, an attacker could try to substitute certain characters inside a malicious payload with equivalent unicode characters. \cite{medium/allypetitt} Such a payload might be able to avoid the filter of the Web Application Firewall that is looking for patterns including the normalized form of the unicode character. As an example, the payload \verb|prompt`${secret}`| could be substituted with the payload \verb|prompt`\uFE69{secret}`|. U+FE69 is named "Small Dollar Sign" and its decomposition form is U+0024, which is the regular Dollar Sign. \cite{comp/uni}
Using the JavaScript function "normalize", the string containing the Small Dollar Sign: \verb|"\uFE69"| can be decomposed using Compatibility Decomposition (NFKD) into a string containing the normal Dollar Sign: \verb|"\u0024"| respectively \verb|"$"| by calling \verb|"\uFE69".normalize("NFKD")|. 

Unicode Normalization is also supported in programming languages different to JavaScript, such as Python. \cite{python/normalization}


\subsubsection{Case alternation}
In order to evade regex filerting by the WAF, the case of a payload can be alternated. \cite{medium/allypetitt}
Modern regex flavors allow the application of modifiers to parts of the regular expression.
One such modifier is \verb|(?i)|. It makes the regex case insensitive. \cite{regex/jan} Only when this modifier is not used, can a payload evade regex filtering using case alternation.

Taking the XSS payload \verb|<script>alert('XSS')</script>| as an example, after applying case alternation, it might result in a payload in the form of: \verb|<sCrIpT>alert('XSS')</sCriPt>|.
Another example is file access on a wrongfully public file.
The Windows file system trests file and directory names as case-insensitive by default. \cite{windows/casesensitive}
On a web server hosted on Windows that exposes a .env file with stored secrets, both urls: \\ \verb|http://127.0.0.1:8000/.env| and \verb|http://127.0.0.1:8000/.enV| \\
are treated equally. 


\subsubsection{Comment interference}
In some context, comments can be used to break up statements. Regarding SQL, the Oracle Database SQL Reference states:
\begin{quote}
	A comment can appear between any keywords, parameters, or punctuation marks in a statement. You can include a comment in a statement in two ways:
	\begin{itemize}
		\item Begin the comment with a slash and an asterisk (/*). Proceed with the text of the comment. This text can span multiple lines. End the comment with an asterisk and a slash (*/). The opening and terminating characters need not be separated from the text by a space or a line break.
		\item Begin the comment with -- (two hyphens). Proceed with the text of the comment. This text cannot extend to a new line. End the comment with a line break.
	\end{itemize}
	\cite{oracle/sqlcomments}
\end{quote}
Ally Petitt suggests using comments inside SQL statements to break up SQL keywords: \verb|?id=1+un/**/ion+sel/**/ect+1,2,3--| \cite{medium/allypetitt}


\subsubsection{Percent encoding}


\subsubsection{Charset alternation}
To indicate the original media type prior to any applied content encoding, the HTTP \verb|Content-Type| representation header is used.
It can be used in requests by the client to tell the server what type of data is actually sent.
\verb|charset| is a possible directive, that specifies the character encoding standard.
It can be supplied with the \verb|Content-Type| header. \cite{http/contenttype}
If a web server is supporting requests in different encoding standards, but the WAF is not configured to parse certain encoding standards, the WAF may not recognize such encoded request as malicious.

The payload \verb|'<script>alert("xss")</script>'| becomes \\
\verb|'L%A2%83%99%89%97%A3n%81%93%85%99%A3M%7F%A7%A2%A2%7F%5DLa%A2%83%99%89%97%A3n'|
after encoding the payload to the charset \verb|IBM037| followed by percent encoding. \\
This differs from the same payload in \verb|UTF-8| followed by percent encoding: \\
\verb|'%3Cscript%3Ealert%28%22xss%22%29%3C%2Fscript%3E'| \\
\verb|UTF-8| encoding is the standard for URIs according to RFC-3986. \cite{rfc3986} \\
Technique by \cite{medium/allypetitt}.
